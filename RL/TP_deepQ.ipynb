{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP apprentissage par renforcement\n",
    "\n",
    "Yann Chevaleyre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations locales les packages python necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --user pyglet==1.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de l'installation\n",
    "\n",
    "Exécutez (et comprenez) le code ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(3):\n",
    "    observation = env.reset()\n",
    "    reward = 0\n",
    "    for t in range(20):\n",
    "        # affichage graphique de l'environnement\n",
    "        env.render()\n",
    "        # tirage d'une action au hasard entre {0,1}\n",
    "        action = np.random.randint(2)\n",
    "        # cet action est faite, et on recupere\n",
    "        # le prochain etat et la recompense\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet environnement `Cart-Pole` consiste à déplacer un chariot pour faire tenir en équilibre une poutre. Plus précisément:\n",
    "* Il y a deux actions : gauche et droite (représentées par 0 et 1)\n",
    "* L'observation reçue (c'est à dire l'état) est un tableau numpy comprenant 4 variables: la position du chariot, la velocite, l'angle a la verticale et la position du haut de la poutre\n",
    "* L'épisode se termine lorsque l'angle de la poutre à la verticale dépasse 12 degré\n",
    "* Les récompenses recues sont égales à 1 sauf si l'angle dépasse 12 degrés.\n",
    "\n",
    "Pour afficher plus d'informations sur cet environnement, tapez `env.env?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice** : dans le code précédent, l'action est choisie au hasard. Modifiez ce code pour que l'action choisie soit \"va a droite\" si l'angle de la poutre est inferieur a 2 degres, et \"va a gauche\" sinon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Première partie: Implémentation du Q-Learning Tabulaire\n",
    "\n",
    "dans cette partie, vous devrez discrétiser l'espace d'état, et implémenter l'algorithme du Q-learning sur le tableau Q(s,a)\n",
    "\n",
    "Pour commencer, voici un rappel de l'algorithme du Q-Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![qlearning_image.png](attachment:qlearning_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va donc devoir discrétiser l'espace d'états.\n",
    "Pour cela, on cree une fonction `discretise(x)`, puis une fonction `observation_vers_etat` qui renvoie un état (nombre entre 0 et N-1) en fonction de l'observation. Lisez le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nval est le pas de discretisation par variable\n",
    "nval = 5         # nombre de valeur discretes qu'une variable peut prendre\n",
    "N    = nval ** 4 # taille de l'espace d'etat\n",
    "\n",
    "def discretise(x,mini,maxi):\n",
    "    # discretise x\n",
    "    # renvoie un entier entre 0 et nval-1\n",
    "    if x < mini:  x = mini\n",
    "    if x > maxi:  x = maxi\n",
    "    return int(np.floor((x-mini)*nval/(maxi-mini+0.0001)))\n",
    "\n",
    "def observation_vers_etat(observation):\n",
    "    pos   = discretise(observation[0],mini=-1,maxi=1)\n",
    "    vel   = discretise(observation[1],mini=-1,maxi=1)\n",
    "    angle = discretise(observation[2],mini=-1,maxi=1)\n",
    "    pos2  = discretise(observation[3],mini=-1,maxi=1)\n",
    "    return pos + vel*nval + angle*nval*nval + pos2*nval*nval*nval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on peut donc récupérer à partir d'une observation le numéro de l'état associé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "s = observation_vers_etat(observation)\n",
    "print(\"le numero de l'etat de depart est le :\",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice **:\n",
    "* Creez un tableau numpy `Q` de dimension `N,2` initialisé aléatoirement. Ce tableau sera utilise dans l'algorithme du q-learning\n",
    "* Implémentez l'algorithme du Q-Learning. Ensuite, ajustez les paramètres de l'algorithme pour qu'il converge correctement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seconde partie: Implémentation du Q-Learning avec approximation de fonction\n",
    "\n",
    "dans cette partie, vous devrez d'abord créer un représentation de chaque état, avant d'implémenter le deep-q-learning\n",
    "\n",
    "**Exercice**: \n",
    "* Creez une fonction `K(x)` qui renvoie `1/(1+x**2)`\n",
    "* Créez un fonction qui transforme l'observation o en une représentation $\\psi(o)$, en utilisant la fonction `K`, comme vu en cours\n",
    "* Implémentez l'algorithme du Q-learning avec approximation de fonction (deep q-learning) \n",
    "* **Bonus**: Implémentez l' *Experience-replay*, et comparez-le a l'algorithme de base (l'algorithme est donné ci-dessous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deepRL-experience-replay.png](attachment:deepRL-experience-replay.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
