{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Lyon - Python text mining - Session 1\n",
    "\n",
    "1. **Text mining with Python**\n",
    "2. Dedicated libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# display options\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "100,000 IMDB movie reviews for Sentiment Analysis:\n",
    "- positive opinion, label `pos`\n",
    "- negative opinion, label `neg`\n",
    "- untagged opinion, label `unsup`\n",
    "\n",
    "Source: kaggle.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text mining with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('imdb_master.csv',\n",
    "                 encoding='latin-1',\n",
    "                 usecols=['review', 'label'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 First insights on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "np.random.seed(1)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review sample\n",
    "print(df.sample(1).iloc[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Exercise 1</b>\n",
    "<ul>\n",
    "    <li>Get the value counts of labels</li>\n",
    "    <li>Apply the `describe()` method to the length of reviews</li>\n",
    "    <li>Perform a seaborn `distplot()` with length of reviews</li>\n",
    "    <li>Get the shortest and the largest review</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load session8/ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Breaking the reviews into words with regular expressions\n",
    "\n",
    "The `re` module from the Python Standard Library provides regular expression matching operations.\n",
    "\n",
    "Regular expressions (called REs, or regex, or regex patterns) are essentially a tiny (but very powerful), highly specialized programming language embedded inside Python and made available through the `re` module.\n",
    "\n",
    "Using this little language, one can specify the rules for the set of possible strings that one wants to match.\n",
    "\n",
    "- `.`: match any character\n",
    "- `*`: match 0 or many repetitions of the preceding pattern\n",
    "- `+`: match 1 or many repetitions of the preceding pattern\n",
    "- `?`: match 0 or 1 repetition of the preceding pattern\n",
    "- `^`: match the start of the string\n",
    "- `$`: match the end of the string\n",
    "- `\\`: in order to use the special characters above as standard ones, one should prefix them with a `\\`\n",
    "\n",
    "It is also possible to list within square brackets the characters that are to match:\n",
    "- `[aeiouy]`: match any vowel\n",
    "- `[^aeiouy]`: do not match any vowel (here the `^` is interpreted as a NOT)\n",
    "- `[A-Z]`: match all characters between `A` and `Z` (several ordered pairs can be used: e.g., for uppercase, lowercase, accentuated letters and digits: `[A-Za-zÀ-ÿ0-9]`)\n",
    "\n",
    "See the ISO 8859-1 (or latin-1) table to get characters intervals: https://en.wikipedia.org/wiki/ISO/IEC_8859-1#Code_page_layout\n",
    "\n",
    "The `findall()` function enables to extract all strings which match a given pattern.\n",
    "\n",
    "The language contains many more powerful functionalities:\n",
    "- shortcuts (e.g., `\\w`, `\\W`)\n",
    "- repetition qualifiers: `*`, `+`, `?`, `{m,n}`\n",
    "- capturing patterns: `(...)`\n",
    "- non capturing patterns: `(?:...)`\n",
    "- backreference to a named group: `(P=name...)`\n",
    "- lookahead assertions: `(?=...)`\n",
    "- negative lookahead assertions: `(?!...)`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Further reading</b>\n",
    "<ul>\n",
    "    <li>https://docs.python.org/3/library/re.html</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import re\n",
    "big_review = df.loc[df['review'].str.len().idxmax(), 'review']\n",
    "big_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1: pattern = 1 uppercase letter\n",
    "pattern = '[A-Z]'\n",
    "re.findall(pattern, big_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2: several uppercase letters\n",
    "pattern = '[A-Z]+'\n",
    "re.findall(pattern, big_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1: several numbers\n",
    "pattern = '[0-9]+'\n",
    "re.findall(pattern, big_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Exercise 2</b>\n",
    "<ul>\n",
    "    <li>Build a pattern to find words starting with an uppercase letter and other letters in lowercase.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load session8/ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find all words compound of uppercase, lowercase, accentuated letters, numbers and possibly an apostrophe followed by `s` or `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words with an apostrophe followed by `s` or `t`.\n",
    "pattern = \"[A-Za-zÀ-ÿ0-9]+'[st]\"\n",
    "re.findall(pattern, big_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words\n",
    "pattern = \"[A-Za-zÀ-ÿ0-9]+(?:'[st])?\"\n",
    "re.findall(pattern, big_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bag-of-words model\n",
    "\n",
    "Computing word frequency in a document could be a hassle:\n",
    "- create a dictionary\n",
    "- split a review into words\n",
    "- for each word:\n",
    "    - if it is not in the dictionary, set the value to 1\n",
    "    - if it is in the dictionary, increase the value with 1\n",
    "    \n",
    "The `Counter` class from the Python standard library `collections` performs the job.\n",
    "\n",
    "The method `update()` takes a list of keys or words and automatically calculates the number of occurences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Further reading</b>\n",
    "<ul>\n",
    "    <li>https://docs.python.org/2/library/collections.html</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Bag of Words model of a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 document\n",
    "c = Counter()\n",
    "c.update(re.findall(pattern, big_review))\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put the result in a Series in order to obtain a vector-like representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the result in a Series object\n",
    "bag_of_words = pd.Series(list(c.values()), index=c.keys())\n",
    "bag_of_words = bag_of_words.sort_values(ascending=False)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Words frequency in all documents\n",
    "\n",
    "Now, we want to compute the words frequency in all documents: i.e., for each word, the number of documents in which it appears.\n",
    "\n",
    "We can apply the `update()` method of the instanciated counter to the full review column.\n",
    "\n",
    "In this schema, we are not interested in the result of the apply method, but in its **side effect** on the counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "c = Counter()\n",
    "df['review'].apply(lambda x: c.update(re.findall(pattern, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dictionary\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the vocabulary\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Putting the results in a Series object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the result in a new DataFrame\n",
    "vocab = pd.Series(list(c.values()), index=c.keys())\n",
    "vocab = vocab.sort_values(ascending=False)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we did not compute the word frequency but the word count. The word `the` appears more than 100K times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Exercise 3</b>\n",
    "<ul>\n",
    "    <li>Modify the code above to get the word frequency: i.e., for each word, the number of documents in which it appears.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load session8/ex3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Finding the context of any word\n",
    "\n",
    "We can notice a strange word: `br`. Let us fond out what is this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Exercise 4</b>\n",
    "<ul>\n",
    "    <li>Select the reviews which contain \"br\".</li>\n",
    "    <li>Print out the reviews with 25 characters before the \"br\" and 25 characters after</li>\n",
    "    <li>Process the reviews to switch the \"br\" to spaces.\n",
    "    <li>Generalize and create a function which finds out all reviews containing a word and print out the results 25 characters before and after.</li>\n",
    "    <li>Then perform few requests:</li>\n",
    "        <ul>\n",
    "            <li>Find a word, e.g. ghost</li>\n",
    "            <li>Find a proper name, e.g. Hitchcock</li>\n",
    "            <li>Find good/bad movie, e.g. </li>\n",
    "            <li>Find good/bad movie, , e.g. good movie, bad movie, not a good movie, not a bad movie</li>\n",
    "            <li>Find Ã</li>\n",
    "        </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load session8/ex4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Cleaning the dataset\n",
    "\n",
    "We can replace all mistaken chars by their appropriate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace character encoding mistakes\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¡', 'á'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã ', 'à'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã ', 'à'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¢', 'â'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã\\xa0', 'à'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¥', 'å'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã£', 'ã'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã»', 'â'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã§', 'ç'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã©', 'é'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¨', 'è'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã«', 'ë'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ãª', 'ê'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¯', 'ï'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã®', 'î'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¬', 'ì'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã\\xad', 'í'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã±', 'ñ'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã³', 'ó'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã²', 'ò'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¶', 'ö'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã´', 'ô'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ãµ', 'õ'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã°', 'ð'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¸', 'ø'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ãº', 'ú'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¹', 'ù'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¼', 'ü'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã½', 'ý'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¿', 'ÿ'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã?', 'Æ'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Ã¦', 'æ'))\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('Â', ''))\n",
    "# br\n",
    "df['review'] = df['review'].apply(lambda x: x.replace('<br />', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Find the top vocabulary in a given context\n",
    "\n",
    "We can use the `find()` function that has been defined above to collect automatically the vocabulary around a word or an expression and then find the top vocabulary which is used.\n",
    "\n",
    "We use an enhanced version of the find function which adds spaces before and after the reviews to deal with words that are at the begining or at the end of a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find word\n",
    "def find(word):\n",
    "    selection = df.loc[df['review'].str.contains(word, regex=False)]\n",
    "    result = selection['review'].apply(lambda x: (' ' * 25 + x + ' ' * 25)[x.find(word):x.find(word) + 50 + len(word)])\n",
    "    return result\n",
    "\n",
    "pattern = '[A-Za-zÀ-ÿ0-9]+(?:\\'[st])?'\n",
    "\n",
    "# top vocabulary\n",
    "def top_voc(s, n=20):\n",
    "    c = Counter()\n",
    "    s.apply(lambda x: c.update(re.findall(pattern, x)))\n",
    "    voc = pd.DataFrame(list(c.items()), columns=['word', 'count'])\n",
    "    voc = voc.nlargest(n, 'count')\n",
    "    return voc['word'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "s = find('Costner')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Duck\n",
    "s = find('Duck')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that some small words are printed out: e.g., a, and, in, is, of, the.\n",
    "\n",
    "### 1.4.3 Collect the stop words\n",
    "\n",
    "In text mining, stop words are words which are filtered out before or after processing of natural language texts.\n",
    "\n",
    "The NLTK module provides a list of such stop words.\n",
    "\n",
    "Of course, stop words depend on the language in which the texts are written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words from nltk\n",
    "if False:\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords_en = set(stopwords.words(\"english\"))\n",
    "else:\n",
    "    import json\n",
    "    with open('stopwords_en.json') as f:\n",
    "        stopwords_en = set(json.load(f))\n",
    "        \n",
    "stopwords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Exercise 5</b>\n",
    "<ul>\n",
    "    <li>Modify the `top_voc()` function such as:</li>\n",
    "        <ul>\n",
    "            <li>Words with less or equal than 2 characters are discarded</li>\n",
    "            <li>Stop words are discarded</li>\n",
    "        </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load session8/ex5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change also the pattern, so that it includes `'d`, `'ll`, `'re` and `'ve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended pattern\n",
    "pattern = '[A-Za-zÀ-ÿ0-9]+(?:\\'(?:d|ll|re|s|t|ve))?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_voc(s, n=20):\n",
    "    c = Counter()\n",
    "    s.apply(lambda x: c.update(re.findall(pattern, x.lower())))\n",
    "    voc = pd.DataFrame(list(c.items()), columns=['word', 'count'])\n",
    "    voc = voc.loc[(voc['word'].str.len() > 2) & ~voc['word'].isin(stopwords_en)]\n",
    "    voc = voc.nlargest(n, 'count')\n",
    "    return voc['word'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "s = find('Costner')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Duck\n",
    "s = find('Duck')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Daffy Duck\n",
    "s = find('Daffy Duck')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Donald Duck\n",
    "s = find('Donald Duck')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 007\n",
    "s = find(' 007 ')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Pierce Brosnan\n",
    "s = find('Pierce Brosnan')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Sean Connery\n",
    "s = find('Sean Connery')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Simpson\n",
    "s = find('Simpson')\n",
    "top_voc(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Naive Bayes Classification\n",
    "\n",
    "The dataset countains 50K labelled documents, 25K positive and 25K negative.\n",
    "\n",
    "We are going to implement a Naive Bayes Classification based on the Bag of Words model.\n",
    "\n",
    "Taking a document `d` we want to compute the probability that the document is in the class `C` (here `pos` or `neg`), i.e., probability of `C` class given a document `d`.\n",
    "\n",
    "Without entering into details, this calculation requires some maths and assumptions:\n",
    "\n",
    "- **Bayes Theorem** application: which enables to pass from the probability of `C` class given a document `d` to the probability of `d` document given a class `C`\n",
    "\n",
    "- **Bag of Words** assumption: a document is represented by its words in any order, which enables to pass from the probability of `d` document given a class `C` to the probability of all words included in the document given a class `C`\n",
    "\n",
    "- **Conditional independance** assumption: probability of words are independent given a class, which enables to pass from the probability of all words included in the document given a class `C` to the product of the probabilities of each word included in the document given a class `C`\n",
    "\n",
    "- To estimate the probability of a word given a class, we can use the **document frequency** (count of documents in the class containing the word divided by the number of documents in the class), or the **maximum likehood** (count of the word in the class divided by the total number of words in the class), or the **Laplace smoothing** by adding 1 in order to avoid factors with $0$ (count of the word in the class plus one divided by the total number of words in the class plus the number of different words in the class)\n",
    "\n",
    "- Then we use the **log** function to transform products into sums, therefore probability into score\n",
    "\n",
    "- In our case, for a review, we will compute the ratio between the score being positive and the one of being negative\n",
    "\n",
    "Further readings:\n",
    "- Naive Bayes classifier: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "- Text Classification in NLP -  Naive Bayes: https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: separate the data into a train and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts of \"label\"\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the scikit-learn `train_test_split()` function to split the dataset into a train set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and apply the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df.loc[df['label']!='unsup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train value counts\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test value counts\n",
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: collect the vocabulary from positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute document frequency\n",
    "pattern = '[A-Za-zÀ-ÿ0-9]+(?:\\'(?:d|ll|re|s|t|ve))?'\n",
    "\n",
    "def get_document_frequency(df):\n",
    "    c = Counter()\n",
    "    df['review'].apply(lambda x: c.update(set(re.findall(pattern, x.lower()))))\n",
    "    vocab = pd.Series(list(c.values()), index=c.keys())\n",
    "    vocab = vocab.sort_values(ascending=False)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency for documents labelled \"pos\"\n",
    "vocab_pos = get_document_frequency(df_train.loc[df['label']=='pos'])\n",
    "vocab_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency for documents labelled \"neg\"\n",
    "vocab_neg = get_document_frequency(df_train.loc[df['label']=='neg'])\n",
    "vocab_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: compute a frequency ratio between positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency ratio pos / neg transformed by log\n",
    "var = vocab_pos.div(vocab_neg, fill_value=1)\n",
    "var = np.log(var)\n",
    "var = var.to_dict()\n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: compute the score of any text by adding log of ratios for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = sum of log of ratio (log has been already computed)\n",
    "def compute_score(review):\n",
    "    words = re.findall(pattern, review.lower())\n",
    "    score = 0\n",
    "    for word in words:\n",
    "        score += var.get(word, 0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of a single word\n",
    "compute_score('amazing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of a single word\n",
    "compute_score('horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of a review\n",
    "compute_score(big_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: score on the train dataset\n",
    "- compute the score of all reviews from train dataset\n",
    "- plot them for positive and negative labels\n",
    "- compute the contingency table in absolute value and in %\n",
    "- compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train['score'] = df_train['review'].apply(compute_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train.loc[df_train['label']=='pos', 'score'])\n",
    "sns.distplot(df_train.loc[df_train['label']=='neg', 'score']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table in absolute value\n",
    "tab = pd.crosstab(df_train.loc[df['label']!='unsup', 'score'] > 0, df_train.loc[df['label']!='unsup', 'label'])\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table in %\n",
    "pd.crosstab(df_train.loc[df['label']!='unsup', 'score'] > 0, df_train.loc[df['label']!='unsup', 'label'], normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "acc = (tab.iloc[0,0] + tab.iloc[1,1])/tab.sum().sum()\n",
    "print('accuracy: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: score on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Exercise 6</b>\n",
    "<ul>\n",
    "    <li>compute the score of all reviews from train dataset</li>\n",
    "    <li>plot them for positive and negative labels</li>\n",
    "    <li>compute the contingency table in absolute value and in %</li>\n",
    "    <li>compute the accuracy</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load session8/ex6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Exercise 7</b>\n",
    "<ul>\n",
    "    <li>Modify the `get_document_frequency()` function such as stop words are discarded</li>\n",
    "    <li>Modify the `compute_score()` function such as the computation is perform only once per word</li>\n",
    "    <li>Re-run the whole process and compare scores</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load session8/ex7.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
