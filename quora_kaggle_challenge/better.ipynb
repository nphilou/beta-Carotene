{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, CuDNNLSTM, Bidirectional\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "train_df \u003d pd.read_csv(\"train.csv\")\ntest_df \u003d pd.read_csv(\"test.csv\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "maxlen \u003d 72\n",
        "\n",
        "train_df[\"question_text\"] \u003d train_df[\"question_text\"].str.lower()\n",
        "test_df[\"question_text\"] \u003d test_df[\"question_text\"].str.lower()\n",
        "\n",
        "X \u003d train_df[\"question_text\"].fillna(\"_NA\").values\n",
        "X_test \u003d test_df[\"question_text\"].fillna(\"_NA\").values\n",
        "\n",
        "tokenizer \u003d Tokenizer()\n",
        "tokenizer.fit_on_texts(list(X))\n",
        "X \u003d tokenizer.texts_to_sequences(X)\n",
        "X_test \u003d tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X \u003d pad_sequences(X, maxlen\u003dmaxlen)\n",
        "X_test \u003d pad_sequences(X_test, maxlen\u003dmaxlen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "Y \u003d train_df[\u0027target\u0027].values\n",
        "X_train, X_val, y_train, y_val \u003d train_test_split(X, Y, test_size\u003d0.2, random_state\u003d42, stratify\u003dY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Improvement 1\n",
        "\n",
        "We can improve the simple Keras architecture by using pre-trained embedding weights from GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "voc_size, emb_dim \u003d 50000, 100\n",
        "\n",
        "embeddings_index \u003d dict()\n",
        "f \u003d open(\u0027glove.840B.300d/glove.840B.300d.txt\u0027)\n",
        "    \n",
        "for line in f:\n",
        "    values \u003d line.split()\n",
        "    word \u003d \u0027\u0027.join(values[:-emb_dim])\n",
        "    coefs \u003d np.asarray(values[-emb_dim:], dtype\u003d\u0027float32\u0027)\n",
        "    embeddings_index[word] \u003d coefs\n",
        "\n",
        "f.close()\n",
        "\n",
        "embedding_matrix \u003d np.zeros((voc_size, emb_dim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index \u003e voc_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector \u003d embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] \u003d embedding_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Improvement 2\n",
        "\n",
        "We add Dropout, Conv1D, Maxpooling1D, CuDNNLSTM to reduce train time (but some of them decreased accuracy)\n",
        "\n",
        "Batch size was increased too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "embedding_18_input (InputLayer) (None, 72)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 72)           0           embedding_18_input[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 72)           0           embedding_18_input[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "sequential_18 (Sequential)      (None, 1)            5034369     lambda_35[0][0]                  \n",
            "                                                                 lambda_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Concatenate)          (None, 1)            0           sequential_18[1][0]              \n",
            "                                                                 sequential_18[2][0]              \n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "Total params: 5,034,369\n",
            "Trainable params: 5,034,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "1044897/1044897 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 47s 45us/step - loss: 0.1221 - acc: 0.9529\n",
            "Epoch 2/3\n",
            "1044897/1044897 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 44s 42us/step - loss: 0.0982 - acc: 0.9602\n",
            "Epoch 3/3\n",
            "1044897/1044897 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 44s 42us/step - loss: 0.0830 - acc: 0.9664\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\u003ckeras.callbacks.History at 0x7fb7dc709668\u003e"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model \u003d Sequential()\n",
        "model.add(Embedding(voc_size, emb_dim, input_length\u003dmaxlen, weights\u003d[embedding_matrix]))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv1D(64, 5, activation\u003d\u0027relu\u0027))\n",
        "model.add(MaxPooling1D(pool_size\u003d4))\n",
        "# model.add(LSTM(10))\n",
        "model.add(Bidirectional(CuDNNLSTM(32)))\n",
        "model.add(Dense(1, activation\u003d\u0027sigmoid\u0027))\n",
        "model \u003d multi_gpu_model(model, gpus\u003d2)\n",
        "model.compile(loss\u003d\u0027binary_crossentropy\u0027, optimizer\u003d\u0027adam\u0027, metrics\u003d[\u0027accuracy\u0027])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs\u003d3, batch_size\u003d256)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Time per epoch:\n",
        "\n",
        "- Simple LSTM: 17 minutes\n",
        "- Optimized LSTM: 45 seconds (22x faster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 95.40%\n"
          ]
        }
      ],
      "source": [
        "scores \u003d model.evaluate(X_val, y_val, verbose\u003d0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "preds \u003d model.predict(X_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[239076,   5987],\n",
              "       [  6022,  10140]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred \u003d np.where(preds \u003e 0.5, 1, 0)\n",
        "confusion_matrix(y_val, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6280776735111028"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(y_val, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We can in fact reach a better f1-score with less train time with these two improvements, we can also make more in-deep preprocessing by adding TFIDF features, process english words contractions, mispellings and exaggerated word lenghts (\"damnnn\" instead of \"damn\"), using the mean of GloVe, Google News and paragram embeddings or try another architecture (GRU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyCharm (beta-Carotene)",
      "language": "python",
      "name": "pycharm-11c4cb55"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "stem_cell": {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}