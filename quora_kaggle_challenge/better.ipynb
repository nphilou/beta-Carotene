{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, CuDNNLSTM, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 72\n",
    "\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n",
    "\n",
    "X = train_df[\"question_text\"].fillna(\"_NA\").values\n",
    "X_test = test_df[\"question_text\"].fillna(\"_NA\").values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = train_df['target'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement 1\n",
    "\n",
    "We can improve the simple Keras architecture by using pre-trained embedding weights from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voc_size, emb_dim = 50000, 100\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.840B.300d/glove.840B.300d.txt')\n",
    "    \n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-emb_dim])\n",
    "    coefs = np.asarray(values[-emb_dim:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((voc_size, emb_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > voc_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement 2\n",
    "\n",
    "We add Dropout, Conv1D, Maxpooling1D, CuDNNLSTM to reduce train time (but some of them decreased accuracy)\n",
    "\n",
    "Batch size was increased too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_18_input (InputLayer) (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 72)           0           embedding_18_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 72)           0           embedding_18_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sequential_18 (Sequential)      (None, 1)            5034369     lambda_35[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Concatenate)          (None, 1)            0           sequential_18[1][0]              \n",
      "                                                                 sequential_18[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 5,034,369\n",
      "Trainable params: 5,034,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1044897/1044897 [==============================] - 47s 45us/step - loss: 0.1221 - acc: 0.9529\n",
      "Epoch 2/3\n",
      "1044897/1044897 [==============================] - 44s 42us/step - loss: 0.0982 - acc: 0.9602\n",
      "Epoch 3/3\n",
      "1044897/1044897 [==============================] - 44s 42us/step - loss: 0.0830 - acc: 0.9664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7dc709668>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, emb_dim, input_length=maxlen, weights=[embedding_matrix]))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "# model.add(LSTM(10))\n",
    "model.add(Bidirectional(CuDNNLSTM(32)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model = multi_gpu_model(model, gpus=2)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time per epoch:\n",
    "\n",
    "- Simple LSTM: 17 minutes\n",
    "- Optimized LSTM: 45 seconds (22x faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.40%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[239076,   5987],\n",
       "       [  6022,  10140]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.where(preds > 0.5, 1, 0)\n",
    "confusion_matrix(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6280776735111028"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can in fact reach a better f1-score with less train time with these two improvements, we can also make more in-deep preprocessing by adding TFIDF features, process english words contractions, mispellings and exaggerated word lenghts (\"damnnn\" instead of \"damn\"), using the mean of GloVe, Google News and paragram embeddings or try another architecture (GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (beta-Carotene)",
   "language": "python",
   "name": "pycharm-11c4cb55"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
